---
title: "PCA - Multiple Linear Regression"
author: "Hector Gavilanes"
format: html
editor: visual
self-contained: true
execute: 
  warning: false
  message: false
---

## Libraries

```{r setup, echo=FALSE}
# Install libraries if needed
# install.packages("caret")
# install.packages("corrplot")
# install.packages("Metrics")
# install.packages("caTools")
# install.packages("EnvStats")
# install.packages("car")
# install.packages("corrr")
# install.packages("ggcorrplot")
# install.packages("FactoMineR")
# install.packages("factoextra")
# install.packages("pls")
# install.packages("e1071")
#install.packages("xgboost")
```

```{r setup, echo = TRUE, warnings = FALSE, message = FALSE}}
# Load necessary libraries
library(tidyr)    # for handling missing values
library(EnvStats) # for rosnerTest
library(caTools)
library(caret)
library(corrplot)
library(Metrics)
library(car)        # for outliers test
library(corrr)      # correlation matrix
library(ggcorrplot) # correlation graph
library(FactoMineR) # PCA analysis
library(factoextra) # PCA plots
library(pls)        # PC regression
library(e1071)      # to fit transform PCA  
#library(xgboost)  
```

# 01 - Data Preparation

```{r}
# clear environment
rm(list = ls())

# Load dataset
train_original <- read.csv('../dataset/DFC_STATE.csv')

# Make a working copy
train_data <- train_original

tail(train_data)

# Constant seed
my_seed = 95
```

## Impute Missing Values

-   Replace missing values with the median.

```{r}
# Count the number of missing values in each column
colSums(is.na(train_data))

# Remove categorical columns
train_data$State <- NULL

# Impute missing values with the mean
for (col in colnames(train_data)) {
  mean_value <- mean(train_data[[col]], na.rm = TRUE)
  train_data[[col]] <- ifelse(is.na(train_data[[col]]), mean_value, train_data[[col]])
}
```

```{r}
# Remove rows with missing values
train_data <- na.omit(train_data)

# Count the number of missing values in each column
colSums(is.na(train_data))

# Get duplicated rows
train_data[duplicated(train_data), ]

# Summary statistics of the data
summary(train_data)

# Summary statistics of categorical variables
summary(train_data[, sapply(train_data, is.character)])
```

```{r}
# Check dataset structure
str(train_data)
```

# 02 - Feature Scaling

```{r}
# Find the index position of the target feature 
target_index <- grep("Survival..As.Expected..STATE.", 
                     colnames(train_data))
```

```{r}
# Standardization Numerical Features
train_data_sc <- scale(train_data[-target_index])

head(train_data_sc)
```

# 03 - PCA Analysis

## Outliers

-   There are no outliers in the data frame.

```{r}
# Plot a boxplot to visualize potential outliers
boxplot(train_data_sc, main = "Boxplot of Values")
```

## Correlations

-   There are high correlated features.

-   Multicollinearity is present in the data set

```{r}
# Calculate correlations and round to 2 digits
corr_matrix <- cor(train_data_sc)
corr_matrix <- round(corr_matrix, digits = 2)

# Print names of highly correlated features
high <- findCorrelation(corr_matrix, cutoff = 0.30, names = TRUE)
high

```

# 04 - PCA - Singular Value Decomposition

-   Note: The Spectral Decomposition approach is used with the princomp() function.

```{r}
# Apply PCA using prcomp()
data_pca <- prcomp(train_data_sc)
summary(data_pca)
```

## PCA - Elements

```{r}
# Principal Component scores
pc_scores <- data_pca$x

# Std Deviation of Components
component_sdev <- data_pca$sdev

# Eigenvector
eigenvector <- data_pca$rotation

# Std Deviation & Mean of Variables
data_pca$center # variable std dev
data_pca$scale  # variable mean

```

## PCA - Cumulative Variance

```{r}
# Proportion of variance explained by each PC
variance_explained <- component_sdev^2 / sum(component_sdev^2)

# Cumulative proportion of variance explained
cumulative_variance_explained <- cumsum(variance_explained)
cumulative_variance_explained
```

## PCA - Number of Principal Components

-   We can conclude that with 9 Principal Components, 86% of the variance is explained.

```{r}
# Retain components that explain a percentage of the variance
num_components <- which(cumulative_variance_explained >= 0.86)[1]

# Select the desired number of principal components
selected_pcs <- pc_scores[, 1:num_components]
selected_pcs
```

# 05 - Visualization

## Scree Plot - Cumulative Variance Explained

```{r}
fviz_eig(data_pca, addlabels = TRUE)
```

## Biplot

```{r}
fviz_pca_biplot(data_pca, 
                geom = c("point", "arrow"),
                geom.var = "arrow")
```

```{r}
# Control variable colors using their contributions
fviz_pca_var(data_pca, col.var = "contrib",
   gradient.cols = c("white", "blue", "red"),
   geom.var = "arrow",
   ggtheme = theme_minimal())
```

# 06 - Model Building

## Standardization

```{r}
# Standardization Numerical Features
train_data_sc <- scale(train_data)
```

## Data Splitting into Training & Test set

```{r}
set.seed(my_seed)  
is.atomic(train_data_sc)  
# Convert vectors to dataframe 
train_data_sc <- as.data.frame(train_data_sc)  
# Create Target y-variable 
y <- train_data_sc$Survival..As.Expected..STATE.  
# Split the data into training and test sets 
split <- sample.split(y, SplitRatio = 0.7) 
training_set <- subset(train_data_sc, split == TRUE) 
test_set <- subset(train_data_sc, split == FALSE) 
```

## Create Feature X-matrix & Target y-variable

```{r}
X_train <- training_set[-target_index] 
y_train <- training_set$Survival..As.Expected..STATE. 
X_test <- test_set[-target_index] 
y_test <- test_set$Survival..As.Expected..STATE. 
```

**Principal Components Regression** -- PCA is used to calculate principal components that can then be used in principal components regression. This type of regression is often used when multicollinearity exists between predictors in a data set.

```{r}
# Perform Principal Component Analysis (PCA) preprocessing on the training data
pca <- preProcess(X_train, method = 'pca', pComp = 8)

# Apply PCA transformation to the training set
training_set <- predict(pca, training_set)

# Reorder columns, moving the first principal component to the end
training_set <- training_set[c(2:9, 1)]

# Apply PCA transformation to the test set
test_set <- predict(pca, test_set)

# Reorder columns in the test set, following the same order as in the training set
test_set <- test_set[c(2:9, 1)]

```

## Linear Regression - 8 Principal Components

```{r}
set.seed(my_seed)

# Fit a multiple linear regression model
reg_model <- lm(Survival..As.Expected..STATE. ~ ., data = training_set)

# Print a summary of the regression model
summary(reg_model)
```

## PCA Linear Regression - 2 Principal Components

```{r}
# Create a subset with 2 principal components
significant_pcs = c(1,2,9)
train_pca <- training_set[, significant_pcs]
```

```{r}
set.seed(my_seed)

# Fit a multiple linear regression model
reg_model <- lm(Survival..As.Expected..STATE. ~ ., 
                data = train_pca)

# Print a summary of the regression model
summary(reg_model)
```

## Repeated Cross-Validation

```{r}
# reproducible random sampling
set.seed(my_seed)

# Repeated cross-validation with 5 folds, 3 repetitions
train_control <- trainControl(method = "repeatedcv",
							number = 5, repeats = 3)

# training the model 
model_cv_repeat <- train(Survival..As.Expected..STATE. ~ ., 
                         data = train_pca,
                         method = "lm",
                         trControl = train_control)

# Print Model Performance
print(model_cv_repeat)

```

## Cross-Validation

```{r}
# reproducible random sampling
set.seed(my_seed)

# Cross-validation with 5 folds
train_control <- trainControl(method = "cv",
							number = 5)

# training the model 
model_cv <- train(Survival..As.Expected..STATE. ~ ., 
                  data = train_pca,
                  method = "lm",
                  trControl = train_control)

# Print Model Performance
print(model_cv)
```

# 07 - Predictions

```{r}
y_pred = predict(reg_model, newdata = test_set[-9])

```

```{r}
#PRESS - predicted residual sums of squares

PRESS <- function(linear.model) {
  #' calculate the predictive residuals
  pr <- residuals(linear.model)/(1-lm.influence(linear.model)$hat)
  #' calculate the PRESS
  PRESS <- sum(pr^2)
  
  return(PRESS)
}

pred_r_squared <- function(linear.model) {
  #' Use anova() to get the sum of squares for the linear model
  lm.anova <- anova(linear.model)
  #' Calculate the total sum of squares
  tss <- sum(lm.anova$'Sum Sq')
  # Calculate the predictive R^2
  pred.r.squared <- 1-PRESS(linear.model)/(tss)
  
  return(pred.r.squared)
}

# Calculate predicted R^2
pred_r2 <- pred_r_squared(reg_model)

# Calculate Mean Absolute Error (MAE)
mae <- mean(abs(y_test - y_pred))

# Calculate Mean Squared Error (MSE)
mse <- mean((y_test - y_pred)^2)

# Calculate Root Mean Squared Error (RMSE)
rmse <- sqrt(mse)

# Calculate R-squared (R²)
sst <- sum((y_test - mean(y_test))^2)  # Total sum of squares
ssr <- sum((y_pred - mean(y_test))^2)  # Regression sum of squares
```

```{r}
# Print the evaluation metrics
cat("Mean Absolute Error (MAE):", mae, "\n")
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("Predicted R^2:", pred_r2, "\n")

```

```{r}
y_pred
```

```{r}
y_test
```

# 08 - Training Conclusion

In conclusion, this project has demonstrated the effectiveness of Principal Component Analysis (PCA) in dimensionality reduction with the following key points:

-   PCA was able to reduce from 37 features down to just 2 principal components.

-   The best score of R\^2 = 96.57% is from Multiple Linear Regression with 2 Principal Components.

-   The repeated K-fold cross-validation provided results of R\^2 = 95.50%.

-   The predicted R\^2 = 95.38 using the Multiple Linear Regression with 2 Principal Components.

-   The model has not been exposed to unseen data to asses its robustness, and reliability.
