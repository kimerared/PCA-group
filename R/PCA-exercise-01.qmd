---
title: "PCA-example-01"
author: "Hector Gavilanes"
format: html
editor: visual
self-contained: true
execute: 
  warning: false
  message: false
---

## Libraries

```{r}
# Install libraries if needed
#install.packages("caret")
#install.packages("corrplot")
#install.packages("Metrics")
#install.packages("caTools")
#install.packages("EnvStats")
#install.packages("car")
#install.packages("corrr")
#install.packages("ggcorrplot")
#install.packages("FactoMineR")

```

```{r setup, echo = TRUE, warnings = FALSE, message = FALSE}}
# Load necessary libraries
library(tidyr)    # for handling missing values
library(EnvStats) # for rosnerTest
library(caTools)
library(caret)
library(corrplot)
library(Metrics)
library(car)      # for outliers test
library(corrr)    # correlation matrix
library(ggcorrplot) # correlation graph
library(FactoMineR) # PCA analysis
  
```

# 01 - Data Preparation

```{r}
# clear environment
rm(list = ls())

# Load dataset
train_original <- read.csv('../dataset/DFC_STATE.csv')

# Make a working copy
train_data <- train_original

tail(train_data)
```

## Impute Missing Values

\- Replace missing values with the median.

```{r}
# Count the number of missing values in each column
colSums(is.na(train_data))

# Remove unnecessary columns
train_data$State <- NULL

# Loop through all columns and impute missing values with the median
for (col in colnames(train_data)) {
  median_value <- median(train_data[[col]], na.rm = TRUE)
  train_data[[col]] <- ifelse(is.na(train_data[[col]]), median_value, train_data[[col]])
}
```

```{r}
# Remove rows with missing values
train_data <- na.omit(train_data)

# Count the number of missing values in each column
colSums(is.na(train_data))

# Get duplicated rows
train_data[duplicated(train_data), ]

# Summary statistics of the data
summary(train_data)

# Summary statistics of categorical variables
summary(train_data[, sapply(train_data, is.character)])
```

## Standardization

```{r}
# Scaling Numerical Features
train_data_sc <- scale(train_data)

head(train_data_sc)
```

## Outliers

```{r}
# Create a sample dataset (replace this with your own dataset)
set.seed(95)

# Plot a boxplot to visualize potential outliers
boxplot(train_data_sc, main = "Boxplot of Values")

```

```{r}
# Test for outliers
outliers <- rosnerTest(train_data_sc,
  k = 3
)
outliers
```

## Correlations

```{r}
corr_matrix <- cor(train_data_sc)
ggcorrplot(corr_matrix)
```

## Splitting data into Features and Target

```{r}
is.atomic(train_data_sc)

# Convert vectors to dataframe
train_data_sc <- as.data.frame(train_data_sc)

# Create y matri
y <- train_data_sc$Survival..As.Expected..STATE.

# Split the data into training and test sets
split <- sample.split(y, SplitRatio = 0.7)
training_set <- train_data_sc[split, ]
test_set <- train_data_sc[!split, ]

# Remove unnecessary columns
cols_to_remove <- c("Survival- As Expected (STATE)")
training_set <- training_set[, !(names(training_set) %in% cols_to_remove)]
test_set <- test_set[, !(names(test_set) %in% cols_to_remove)]

```

```{r}
# Fit a multiple regression model
model <- lm(training_set$Survival..As.Expected..STATE. ~ ., training_set)

# Print a summary of the model
summary(model)

```

# 03 - PCA

```{r}
# Step 2: Apply PCA using prcomp()
pca_result <- prcomp(training_set, center = TRUE, scale. = TRUE)

# Principal components (PCs)
pcs <- pca_result$x

# Standard deviations (variances) of the PCs
std_dev <- pca_result$sdev

# Loadings (correlations between original variables and PCs)
loadings <- pca_result$rotation

# Proportion of variance explained by each PC
variance_explained <- pca_result$sdev^2 / sum(pca_result$sdev^2)

# Cumulative proportion of variance explained
cumulative_variance_explained <- cumsum(variance_explained)
```

## Components

```{r}
print(cumulative_variance_explained)
```

```{r}
# Retain components that explain a percentage (%) of the variance
num_components <- which(cumulative_variance_explained >= 0.86)[1]

# Step 5: Select the desired number of principal components
selected_pcs <- pcs[, 1:num_components]
```

```{r}
print(selected_pcs)
```

# 04 - Visualization

```{r}
# Step 1: Extract the eigenvalues
eigenvalues <- pca_result$sdev^2

# Calculate the proportion of variance explained by each PC
variance_proportion <- eigenvalues / sum(eigenvalues)

# Step 2: Create a scree plot
scree_data <- data.frame(PrincipalComponent = 1:length(eigenvalues), Eigenvalue = eigenvalues, VarianceProportion = variance_proportion)

scree_plot <- ggplot(data = scree_data, aes(x = PrincipalComponent)) +
  geom_bar(aes(y = Eigenvalue), stat = "identity", fill = "magenta", width = 0.5) +
  geom_line(aes(y = Eigenvalue), color = "blue") +
  geom_text(aes(y = Eigenvalue, label = paste0(round(VarianceProportion * 100, 2), "%")), vjust = -0.5, size = 3) +
  labs(x = "Principal Component", y = "Eigenvalue", title = "Scree Plot") +
  theme_minimal()

# Step 3: Display the scree plot
print(scree_plot)
```

```{r}
# Step 1: Extract the eigenvalues
eigenvalues <- pca_result$sdev^2

# Step 2: Create a scree plot
scree_data <- data.frame(PrincipalComponent = 1:length(eigenvalues), Eigenvalue = eigenvalues)
scree_plot <- ggplot(data = scree_data, aes(x = PrincipalComponent, y = Eigenvalue)) +
  geom_point(shape = 19, size = 3) +
  geom_line() +
  labs(x = "Principal Component", y = "Eigenvalue", title = "Scree Plot")

# Step 3: Display the scree plot
print(scree_plot)
```

```{r}
# Plot the data using the first two principal components
# library(ggplot2)
ggplot(data = as.data.frame(selected_pcs), aes(x = PC1, y = PC2)) +
geom_point()
```

```{r}
# Perform Kernel PCA
library(kernlab)
kpca <- kpca(~., data = data.frame(X_train), kernel = "vanilladot", features = 8)
X_train_kpca <- as.data.frame(predict(kpca, data.frame(X_train)))
X_test_kpca <- as.data.frame(predict(kpca, data.frame(X_test)))

# Train a linear regression model on the Kernel PCA components
linear_reg_kpca <- lm(y_train ~ ., data = data.frame(y_train, X_train_kpca))
y_pred_kpca <- predict(linear_reg_kpca, newdata = data.frame(X_test_kpca))

# Evaluate the Kernel PCA model
mse_kpca <- mse(y_test, y_pred_kpca)
r2_kpca <- R2(y_test, y_pred_kpca)

cat("Mean Squared Error (Kernel PCA):", mse_kpca, "\n")
cat("R-squared (Kernel PCA):", r2_kpca, "\n")

# Calculate the adjusted R-squared for Kernel PCA
n <- length(y)  # Number of observations
p <- ncol(X_train_kpca)  # Number of predictors (independent variables/features)
adjusted_r_squared_kpca <- 1 - ((1 - r2_kpca) * (n - 1)/ (n - p - 1))

cat("Adjusted R-squared (Kernel PCA):", adjusted_r_squared_kpca, "\n")

# Extract coefficients and intercept from the Kernel PCA model
coefficients_kpca <- coef(linear_reg_kpca)
intercept_kpca <- coefficients_kpca[1]

cat("Coefficients (Kernel PCA):", coefficients_kpca[-1], "\n")
cat("Intercept (Kernel PCA):", intercept_kpca, "\n")

# Plot Kernel PCA components
library(ggplot2)
X_train_kpca_plot <- data.frame(PC1 = X_train_kpca$PC1, PC2 = X_train_kpca$PC2, Target = y_train)
ggplot(X_train_kpca_plot, aes(x = PC1, y = PC2, color = Target)) +
  geom_point() +
  xlab('First Principal Component (Kernel PCA)') +
  ylab('Second Principal Component (Kernel PCA)') +
  ggtitle('Kernel PCA - First Two Principal Components') +
  scale_color_gradient(low = "blue", high = "red")

```
