---
title: "PCA-example-01"
author: "Hector Gavilanes"
format: html
editor: visual
self-contained: true
execute: 
  warning: false
  message: false
---

## Libraries

```{r}
# Install libraries if needed
# install.packages("caret")
# install.packages("corrplot")
# install.packages("Metrics")
# install.packages("caTools")
# install.packages("EnvStats")
# install.packages("car")
# install.packages("corrr")
# install.packages("ggcorrplot")
# install.packages("FactoMineR")
# install.packages("factoextra")
# install.packages("pls")
# install.packages("e1071")
install.packages("xgboost")
```

```{r setup, echo = TRUE, warnings = FALSE, message = FALSE}}
# Load necessary libraries
library(tidyr)    # for handling missing values
library(EnvStats) # for rosnerTest
library(caTools)
library(caret)
library(corrplot)
library(Metrics)
library(car)        # for outliers test
library(corrr)      # correlation matrix
library(ggcorrplot) # correlation graph
library(FactoMineR) # PCA analysis
library(factoextra) # PCA plots
library(pls)        # PC regression
library(e1071)      # to fit transform PCA  
library(xgboost)  
```

# 01 - Data Preparation

```{r}
# clear environment
rm(list = ls())

# Load dataset
train_original <- read.csv('../dataset/DFC_STATE.csv')

# Make a working copy
train_data <- train_original

tail(train_data)

# Constant seed
my_seed = 95
```

## Impute Missing Values

-   Replace missing values with the median.

```{r}
# Count the number of missing values in each column
colSums(is.na(train_data))

# Remove unnecessary columns
train_data$State <- NULL

# Loop through all columns and impute missing values with the mean
for (col in colnames(train_data)) {
  mean_value <- mean(train_data[[col]], na.rm = TRUE)
  train_data[[col]] <- ifelse(is.na(train_data[[col]]), mean_value, train_data[[col]])
}
```

```{r}
# Remove rows with missing values
train_data <- na.omit(train_data)

# Count the number of missing values in each column
colSums(is.na(train_data))

# Get duplicated rows
train_data[duplicated(train_data), ]

# Summary statistics of the data
summary(train_data)

# Summary statistics of categorical variables
summary(train_data[, sapply(train_data, is.character)])
```

```{r}
# Check dataset structure
str(train_data)
```

# 02 - Feature Scaling

```{r}
# Standardization Numerical Features
train_data_sc <- scale(train_data)

head(train_data_sc)
```

## Data Splitting into Training & Test set

```{r}
set.seed(my_seed)

is.atomic(train_data_sc)

# Convert vectors to dataframe
train_data_sc <- as.data.frame(train_data_sc)

# Create Target y-variable
y <- train_data_sc$Survival..As.Expected..STATE.

# Split the data into training and test sets
split <- sample.split(y, SplitRatio = 0.7)
training_set <- subset(train_data_sc, split == TRUE)
test_set <- subset(train_data_sc, split == FALSE)

```

## Create Feature X-matrix & Target y-variable

```{r}
# Get the column names of the data frame or matrix
col_names <- colnames(training_set)

# Find the index position of the target feature
target_index <- which(col_names == "Survival..As.Expected..STATE.")
target_index
```

```{r}
X_train <- training_set[-29]
y_train <- training_set$Survival..As.Expected..STATE.
X_test <- test_set[-29]
y_test <- test_set$Survival..As.Expected..STATE.

```

# 03 - PCA Analysis

## Outliers

-   There are no outliers in the data frame.

```{r}
# Plot a boxplot to visualize potential outliers
boxplot(train_data_sc, main = "Boxplot of Values")
```

## Correlations

```{r}
# Calculate correlations and round to 2 digits
corr_matrix <- cor(X_train)
corr_matrix <- round(corr_matrix, digits = 2)

# highly correlated features
high <- findCorrelation(corr_matrix, cutoff = 0.30)

# Print names of highly correlated features
names(X_train[, high])
```

## PCA - Correlation matrix

```{r}
# Apply PCA to correlation matrix
data_pca <- princomp(corr_matrix)
summary(data_pca)
```

## PCA - Evaluation

```{r}
data_pca$loadings[, 1:4]
```

### PCA - Components

```{r}
# Get scores
pc_component <- data_pca$scores

# Check 38 components
head(pc_component)

# Demostrate uncorrelated PCs
cor(pc_component)
```

### Covariance matrix Sigma

```{r}
sigma_pca <- var(train_data_sc)

# Eigenvalues and Eigenvector of Sigma
eigen_pca <- eigen(sigma_pca)
eigen_pca
```

### Cumulative Variance

```{r}
# Proportion of variance explained by each PC
variance_explained <- data_pca$sdev^2 / sum(data_pca$sdev^2)

# Cumulative proportion of variance explained
cumulative_variance_explained <- cumsum(variance_explained)
cumulative_variance_explained
```

## Number of Principal Components

-   We can conclude that with 2 Principal Components, 86% of the variance is explained.

```{r}
# Retain components that explain a percentage (%) of the variance
num_components <- which(cumulative_variance_explained >= 0.86)[1]

# Select the desired number of principal components
selected_pcs <- pc_component[, 1:num_components]
selected_pcs
```

# 05 - Visualization

## Scree Plot - Cumulative Variance Explained

```{r}
fviz_eig(data_pca, addlabels = TRUE)
```

## Biplot

```{r}
fviz_pca_biplot(data_pca, 
                geom = c("point", "arrow"),
                geom.var = "arrow")
```

```{r}
# Control variable colors using their contributions
fviz_pca_var(data_pca, col.var = "contrib",
   gradient.cols = c("white", "blue", "red"),
   geom.var = "arrow",
   ggtheme = theme_minimal())
```

# 06 - Model Building

**Principal Components Regression** -- We can also use PCA to calculate principal components that can then be used in principal components regression. This type of regression is often used when multicollinearity exists between predictors in a data set.

```{r}
# Perform Principal Component Analysis (PCA) preprocessing on the training data
pca <- preProcess(X_train, method = 'pca', pComp = 2)

# Apply PCA transformation to the training set
training_set <- predict(pca, training_set)

# Reorder columns, moving the first principal component to the end
training_set <- training_set[c(2:3, 1)]

# Apply PCA transformation to the test set
test_set <- predict(pca, test_set)

# Reorder columns in the test set, following the same order as in the training set
test_set <- test_set[c(2:3, 1)]

```

## Linear Regression

```{r}
set.seed(my_seed)

# Fit a multiple linear regression model
reg_model <- lm(Survival..As.Expected..STATE. ~ ., data = training_set)

# Print a summary of the regression model
summary(reg_model)
```

## Repeated Cross-Validation

```{r}
# reproducible random sampling
set.seed(my_seed)

# Repeated cross-validation with 5 folds, 3 repetitions
train_control <- trainControl(method = "repeatedcv",
							number = 5, repeats = 3)

# training the model 
model_cv_repeat <- train(Survival..As.Expected..STATE. ~ ., data = training_set,
			method = "lm",
			trControl = train_control)

# Print Model Performance
print(model_cv_repeat)

```

## Cross-Validation

```{r}
# reproducible random sampling
set.seed(my_seed)

# Cross-validation with 5 folds
train_control <- trainControl(method = "cv",
							number = 5)

# training the model 
model_cv <- train(Survival..As.Expected..STATE. ~ ., data = training_set,
			method = "lm",
			trControl = train_control)

# Print Model Performance
print(model_cv)
```

# Training Conclusion

-   The best score of 96.69% is from Multiple Linear Regression (reg_model) with 2 Principal Components.

-   The repetead K-fold cross-validation provided results of R\^2 = 95.40%.

# 07 - Predictions

```{r}
y_pred = predict(reg_model, newdata = test_set[-3])

# Calculate Mean Absolute Error (MAE)
mae <- mean(abs(y_test - y_pred))

# Calculate Mean Squared Error (MSE)
mse <- mean((y_test - y_pred)^2)

# Calculate Root Mean Squared Error (RMSE)
rmse <- sqrt(mse)

# Calculate R-squared (R²)
sst <- sum((y_test - mean(y_test))^2)  # Total sum of squares
ssr <- sum((y_pred - mean(y_test))^2)  # Regression sum of squares
r_squared <- 1 - (ssr / sst)

# Print the evaluation metrics
cat("Mean Absolute Error (MAE):", mae, "\n")
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")
cat("R-squared (R²):", r_squared, "\n")

```

```{r}
y_pred
```

```{r}
y_test
```
